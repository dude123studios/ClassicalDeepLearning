{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO TF2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4g6djfUCeBS7eNpoq+yzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedDeepLearning/blob/main/PPO_TF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Ny3TIYnllO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.layers import *\n",
        "import numpy as np\n",
        "import gym\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FmSpRzmoY5R"
      },
      "source": [
        "env = gym.make('Pendulum-v0')\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.observation_space.shape[0]\n",
        "action_bound = [env.action_space.low, env.action_space.high]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCmRpa2Jof0k"
      },
      "source": [
        "def policy_nw(state_shape, action_shape):\n",
        "    inputs = Input(shape=(state_shape,))\n",
        "\n",
        "    x = Dense(128, 'relu')(inputs)\n",
        "    mu = 2 * Dense(action_shape, 'tanh', name='mu')(x)\n",
        "    sigma = Dense(action_shape, tf.keras.activations.softplus, name='sigma')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=[mu, sigma])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9b4nSS7qaU_"
      },
      "source": [
        "def value_nw(state_shape):\n",
        "    return tf.keras.models.Sequential([\n",
        "        Dense(128, 'relu'),\n",
        "        Dense(1)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUpvlXod7v0U"
      },
      "source": [
        "pi = policy_nw(state_shape, action_shape)\n",
        "oldpi = policy_nw(state_shape, action_shape)\n",
        "v = value_nw(state_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpv1F_NQ-f34"
      },
      "source": [
        "#Define Optimizers\n",
        "pi_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "v_optimizer = tf.keras.optimizers.Adam(2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFonHQ-2DFen"
      },
      "source": [
        "#Hyper Parameters\n",
        "num_episodes = 2000\n",
        "num_timesteps = 200\n",
        "gamma = 0.9\n",
        "zeta = 0.3\n",
        "beta = 0.2\n",
        "epsilon = 0.2\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwzoOy6i9IvW"
      },
      "source": [
        "def update_oldpi():\n",
        "    for (a, b) in zip(oldpi.trainable_variables, pi.trainable_variables):\n",
        "        a.assign(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EGKy-39NG2L"
      },
      "source": [
        "def policy(state):\n",
        "    state = state[np.newaxis, :]\n",
        "    state = tf.convert_to_tensor(state)\n",
        "    mu, sigma = pi(state)\n",
        "    dist = tfd.Normal(mu[0], sigma[0])\n",
        "    action = tf.squeeze(dist.sample(1), axis=0).numpy()\n",
        "    clipped = np.clip(action, action_bound[0], action_bound[1])\n",
        "    return clipped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyPiu5dgQz16"
      },
      "source": [
        "def value(state):\n",
        "    if state.ndim < 2: state = state[np.newaxis, :]\n",
        "    state = tf.convert_to_tensor(state)\n",
        "    return v(state).numpy()[0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G9BWaNn6MFX"
      },
      "source": [
        "#Train function\n",
        "@tf.function\n",
        "def train_networks(state, action, reward, _beta):\n",
        "\n",
        "    #Copies old network\n",
        "    update_oldpi()\n",
        "\n",
        "    #Calculates advantage constant ahead of time for effeciency\n",
        "    advantage_const = reward - v(state)\n",
        "\n",
        "    # Loops update because at first pi/oldpi = 1, and we need to differentiate them more\n",
        "    for _ in range(10):\n",
        "        # Train policy network\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Calculate pi distribution\n",
        "            mu, sigma = pi(state)\n",
        "            dist = tfd.Normal(mu, sigma)\n",
        "            # Sample\n",
        "            pi_prob = dist.prob(action)\n",
        "            \n",
        "            # Calculate oldpi distribution\n",
        "            mu_, sigma_ = oldpi(state)\n",
        "            dist_ = tfd.Normal(mu_, sigma_)\n",
        "            # Sample\n",
        "            oldpi_prob = dist_.prob(action)\n",
        "            \n",
        "            #Calculate KL Divergence in Penalty term\n",
        "            kl_div = tfd.kl_divergence(dist, dist_)\n",
        "            \n",
        "            # Prevent NaN and calculate ratio\n",
        "            ratio = pi_prob/(oldpi_prob + 1e-5)\n",
        "\n",
        "            objective = ratio * advantage_const\n",
        "            \n",
        "            # Clip\n",
        "            clipped = tf.minimum(objective, tf.clip_by_value(ratio, 1-epsilon, 1+epsilon))\n",
        "\n",
        "            pi_loss = -tf.reduce_mean(clipped - _beta * kl_div)\n",
        "        \n",
        "        # Apply gradients\n",
        "        grads = tape.gradient(pi_loss, pi.trainable_variables)\n",
        "        pi_optimizer.apply_gradients(zip(grads, pi.trainable_variables))\n",
        "\n",
        "\n",
        "    # Update beta by update rule\n",
        "    mean_kl = tf.reduce_mean(kl_div)\n",
        "\n",
        "    if mean_kl > 1.5 * zeta:\n",
        "        _beta *= 2.0\n",
        "    elif mean_kl < zeta/1.5:\n",
        "        _beta *= 0.5\n",
        "\n",
        "    #Train value network\n",
        "    with tf.GradientTape() as tape:\n",
        "        advantage = reward - v(state)\n",
        "        v_loss = tf.reduce_mean(tf.square(advantage))\n",
        "\n",
        "    grads = tape.gradient(v_loss, v.trainable_variables)\n",
        "    v_optimizer.apply_gradients(zip(grads, v.trainable_variables))\n",
        "\n",
        "    return _beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiPEX7_gO4LQ"
      },
      "source": [
        "#TRAIN!\n",
        "for i in range(num_episodes):\n",
        "\n",
        "    # Reset state, saved buffer, and return\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards = [], [], []\n",
        "    Return = 0\n",
        "\n",
        "    for t in range(num_timesteps):\n",
        "        \n",
        "        #Select action\n",
        "        action = policy(state)\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        #Save to buffer\n",
        "        episode_states.append(state)\n",
        "        episode_rewards.append(reward)\n",
        "        episode_actions.append(action)\n",
        "\n",
        "        state = next_state\n",
        "        Return += reward\n",
        "\n",
        "        # Train step\n",
        "        if (t+1) % batch_size == 0 or t == num_timesteps-1:\n",
        "            v_s_ = value(state)\n",
        "\n",
        "            # Calculate the disctounted reward from current state\n",
        "            discounted_r = []\n",
        "            for reward in episode_rewards[::-1]:\n",
        "                v_s_ = reward + gamma * v_s_\n",
        "                discounted_r.append(v_s_)\n",
        "            discounted_r.reverse()\n",
        "\n",
        "            #Prepare arrays for trainaing\n",
        "            es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r, np.float32)[:, np.newaxis]\n",
        "            es, ea, er = tf.convert_to_tensor(es), tf.convert_to_tensor(ea), tf.convert_to_tensor(er)\n",
        "\n",
        "            #Train!\n",
        "            beta = train_networks(es, ea, er, beta)\n",
        "            #Reset buffer\n",
        "            \n",
        "            episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print('Episode: {}, Return: {}'.format(i, Return)) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}